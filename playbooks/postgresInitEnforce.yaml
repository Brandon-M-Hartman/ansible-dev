---
### Title: postgresInitEnforce.yaml
### Description: Initializes and/or updates Postgres configs, provisions new/failing standby hosts and starts replication.
### Author: Brandon Hartman
### Date: 6-28-2021
### Notes: This job can be run from AWX or Tower as it doesn't failover by itself, only updates running configs. May interrupt service for a moment if you have postgres restart. You may also use a different data directory. If you do, you'll need to change the relevant portions. I'll make that a variable in the future.

- hosts: postgres
  vars:
    primaryNode: "{{ primary_node_fqdn }}"
    repl_user_username: "{{ repl_user_uname }}"
    accessHosts: "{{ access_hosts.split(',') }}" #List hosts split by a comma, no spaces.
    postgresHosts: "{{ postgres_hosts.split(',') }}" #List hosts split by a comma, no spaces.
    restartPostgres: "{{ restart_postgres | default(false) }}"
    forceStandbyToReinitialize: "{{ forceStandbyReinitialize | default(false) }}"
  vars_prompt:
    - name: repl_user_password
      prompt: "Enter pgsql replication user's password"
      private: yes
  tasks:

    - name: Check if have an existing data directory
      stat:
        path: /var/lib/pgsql/data
      register: datadir

    - name: Enforce configurations in data directory if we already have configuration files.
      block:
      - name: Remove recovery.done file if it exists # Since we're enforcing a recovery.conf from a template, delete any that are already there to ensure idempotency.
        ansible.builtin.file:
          path: /var/lib/pgsql/data/recovery.done
          state: absent

      - name: Remove recovery.conf file it exists and we're a primary node.
        ansible.builtin.file:
          path: /var/lib/pgsql/data/recovery.conf
          state: absent
        when: inventory_hostname in primaryNode #Only run if we are not currently a primary node. Use "in" in case of discrepancy between hostname and FQDN.

      - name: Ensure /var/lib/pgsql/data exists.
        ansible.builtin.file:
          path: /var/lib/pgsql/data
          state: directory
          owner: postgres
          group: postgres
          mode: 0700

      - name: Enforce our host configurations in pg_hba.conf
        become: yes
        template:
          src: templates/pg_hba.conf.j2
          dest: /var/lib/pgsql/data/pg_hba.conf
          group: postgres
          owner: postgres
          mode: 0700

      - name: Enforce listening on all addresses
        lineinfile:
          path: /var/lib/pgsql/data/postgresql.conf
          regexp: '.*listen_addresses.*'
          line: "listen_addresses = '*'"

      - name: Set max_wal_senders to 8 if it's commented
        lineinfile:
          path: /var/lib/pgsql/data/postgresql.conf
          regexp: '^\s*#*\s*max_wal_senders.*'
          line: "max_wal_senders = 8"
          insertbefore: '.*Master Server.*' 

      - name: Comment out wal_senders variable
        lineinfile:
          path: /var/lib/pgsql/data/postgresql.conf
          regexp: '^\s*wal_senders =.*'
          line: "# wal_senders = 8 #Keep this commented out, it causes issues if a node is set to standby but has this variable set. Use max_wal_senders instead."
          insertbefore: '.*Master Server.*'

      - name: Uncomment and enforce logical replication
        lineinfile:
          path: /var/lib/pgsql/data/postgresql.conf
          regexp: '.*wal_level.*'
          line: "wal_level = logical"

      - name: Uncomment and enforce wal_log_hints for streaming of non-critical updates
        lineinfile:
          path: /var/lib/pgsql/data/postgresql.conf
          regexp: '.*wal_log_hints.*'
          line: "wal_log_hints = on"

      - name: Enforce max_wal_size
        lineinfile:
          path: /var/lib/pgsql/data/postgresql.conf
          regexp: '.*max_wal_size.*'
          line: "max_wal_size = 1GB"
          insertbefore: '.*Master Server.*'

      - name: Enable hot_standby in config.
        lineinfile:
          path: /var/lib/pgsql/data/postgresql.conf
          regexp: '.*hot_standby = .*'
          line: "hot_standby = on # This variable is not used unless recovery.conf is present, but the variable can be set on primary nodes without hurting anything."

      - name: Ensure recovery.conf exists and points to correct primary node, but only if we are a standby node.
        become: yes
        template:
          src: templates/recovery.conf.j2
          dest: /var/lib/pgsql/data/recovery.conf
          group: postgres
          owner: postgres
          mode: 0700
        when: inventory_hostname not in primaryNode #Only run if we are not currently a primary node. Use "not in" in case of discrepancy between hostname and FQDN.
      when: datadir.stat.exists #Tied to this entire block above

    - name: Touch permissions on pgsql and children
      file:
        path: /var/lib/pgsql
        state: directory
        mode: 0700
        owner: postgres
        group: postgres
        recurse: yes
      changed_when: false

    - name: Restart postgres service
      service:
        name: postgresql
        state: restarted
      when: restartPostgres
      ignore_errors: "{{ forceStandbyToReinitialize and inventory_hostname not in primaryNode }}" # When we're rebuilding a standby, it may not have a datadir or proper config yet, so we're fine it fails.

    - name: Wait for postgres to come back up
      wait_for:
        port: 5432
        delay: 10
        timeout: 30
      when: restartPostgres 
      ignore_errors: "{{ forceStandbyToReinitialize and inventory_hostname not in primaryNode }}" # When we're rebuilding a standby, it may not have a datadir or proper config yet, so we're fine it fails.

    - name: Only run if we're a primary node
      block:
      - name: Ensure replication user exists with proper permissions on our primary node
        become: yes
        become_user: postgres
        postgresql_user:
          db: postgres
          name: "{{ repl_user_username }}"
          password: "{{ repl_user_password }}" #We always want to do no_log when passing passwords, but the postgres module doesn't support it so it's not present here.
          role_attr_flags: LOGIN,REPLICATION
        ignore_errors: "{{ ansible_check_mode }}" #If we're in check mode and in a workflow, we may be running an alter check on a read-only node, so that *should* fail.
      when: inventory_hostname in primaryNode #Only run if we are currently a primary node. Use "in" in case of discrepancy between hostname and FQDN.

    - name: Only run if we're a standby node.
      block:

      - name: Check replication status
        become: yes
        become_user: postgres
        shell: export PGPASSWORD="{{ repl_user_password }}" && psql -c 'SELECT * FROM pg_stat_wal_receiver LIMIT 1;' -t -0 -A | wc -w
        register: replication_result
        no_log: true #Never log passwords.
        changed_when: False

      - name: Set precheck_replication_ok based on replication result (pre-rebuild)
        set_fact:
          precheck_replication_ok: "{{ 'true' if ( replication_result.stdout | int ) >= 7 else 'false' }}"

      - name: Assert that replication is okay to see if we need to rebuild.
        assert:
          that: precheck_replication_ok
          fail_msg: 'Postgres on "{{ inventory_hostname }}" is NOT replicating. Going to rebuild this node.'
          success_msg: 'Postgres on "{{ inventory_hostname }}" is replicating succcessfully! Only rebuilding if we are being forced to do so.'
        ignore_errors: yes #If this task fails, that's fine, we're going to continue on since we need to rebuild.

      - name: Replication status variables before seeing if we'll exit the task or rebuild the node.
        debug:
          msg:
          - Result of replication check command: "{{ replication_result.stdout }}"
          - Do we have replication?: "{{ precheck_replication_ok }}"
          - Force standby to reinitialize: "{{ forceStandbyToReinitialize }}"
          verbosity:
            2

      - name: Send message and exit if we're exiting due to replication succeeding and not forcing re-initialization.
        block:
        - name: "Exit successfully if replication is fine, unless forced to continue or replication isn't working."
          debug:
            msg: "Replication is working, and we aren't forcing a re-initialize, ending task."
        - meta: end_host
        ignore_errors: "{{ ansible_check_mode }}" #If we're in check mode,, we may be checking replication reception on a primary node, in which case we can safely ignore this error.
        when: precheck_replication_ok and not forceStandbyToReinitialize #replication_status.stdout should have a full SQL record - There isn't really a good way to check this in postgres 10, so just need to make sure this isn't an empty result from above.
        #If we receive a dict back, it was an empty result and replication is failing, thus the != "dict" portion - if status isn't a dict, then we got results back and replication is going.

      - name: Stop postgres service
        service:
          name: postgresql
          state: stopped

      - name: Remove data directory
        file:
          path: /var/lib/pgsql/data
          state: absent

      - name: Perform backup from primary node
        shell: export PGPASSWORD="{{ repl_user_password }}" && pg_basebackup -h "{{ primaryNode }}" -U "{{ repl_user_username }}" -D /var/lib/pgsql/data -P -R -X stream -c fast 2>&1
        become: yes
        become_user: postgres
        #no_log: true #Never log passwords.
        ignore_errors: "{{ ansible_check_mode }}" #If we're in check mode, we may be trying to perform a backup on a primary node, in which case we can safely ignore this error.

      - name: Change recovery.conf to our templated version
        become: yes
        template:
          src: templates/recovery.conf.j2
          dest: /var/lib/pgsql/data/recovery.conf
          group: postgres
          owner: postgres
          mode: 0700

      - name: Touch permissions on pgsql and children
        file:
          path: /var/lib/pgsql
          state: directory
          mode: 0700
          owner: postgres
          group: postgres
          recurse: yes

      - name: Start postgres service
        service:
          name: postgresql
          state: started

      - name: Wait for postgres to come back up
        wait_for:
          port: 5432
          delay: 10

      - name: Check replication status again
        become: yes
        become_user: postgres
        shell: psql -c 'SELECT * FROM pg_stat_wal_receiver LIMIT 1;' -t -0 -A | wc -w
        environment:
          PGPASSWORD: "{{ repl_user_password }}"
        register: replication_result
        no_log: true #Never log passwords.
        changed_when: False
        ignore_errors: "{{ ansible_check_mode }}" #If we're in check mode, we may be checking replication reception on a primary node, in which case we can safely ignore this error.

      - name: Set replication check based on replication result (post-rebuild)
        set_fact:
          postbuild_replication_ok: "{{ 'true' if ( replication_result.stdout | int ) >= 7 else 'false' }}"

      - name: Replication status variables after rebuild
        debug:
          msg:
          - Result of replication check command: "{{ replication_result.stdout }}"
          - Do we have replication?: "{{ precheck_replication_ok }}"
          - Did we force the standby to rebuild?: "{{ forceStandbyToReinitialize }}"
          verbosity:
            2

      - name: Assert that replication is okay after the rebuild
        assert:
          that: postbuild_replication_ok
          fail_msg: 'Postgres on "{{ inventory_hostname }}" is NOT replicating after rebuild. Please investigate further.'
          success_msg: 'Postgres on "{{ inventory_hostname }}" is replicating succcessfully after the rebuild!'
        ignore_errors: yes #We'll continue because we can still continue working with the primary node so we have at least one system running while the standby issue is investigated.
      
      when: inventory_hostname not in primaryNode #Only run if we are not currently a primary node. Use "not in" in case of discrepancy between hostname and FQDN.
